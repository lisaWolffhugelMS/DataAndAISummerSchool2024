{"cells":[{"cell_type":"markdown","source":["## Task 1: Import required libraries and initialize Spark session."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7998d7a4-322b-48b1-82d1-8cea872cd9a3"},{"cell_type":"code","source":["from pyspark.sql.functions import udf, col\n","\n","from synapse.ml.io.http import HTTPTransformer, http_udf\n","\n","from requests import Request\n","\n","from pyspark.sql.functions import lit\n","\n","from pyspark.ml import PipelineModel\n","\n","from pyspark.sql.functions import col\n","\n","import os"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"56a4ab0d-16a5-4a43-b9dd-151ec29c68c7","normalized_state":"finished","queued_time":"2024-08-16T15:50:46.9771445Z","session_start_time":null,"execution_start_time":"2024-08-16T15:50:47.5559489Z","execution_finish_time":"2024-08-16T15:50:50.0049678Z","parent_msg_id":"8668ece4-5bd9-4b0c-bfb3-cf6e8bbb244d"},"text/plain":"StatementMeta(, 56a4ab0d-16a5-4a43-b9dd-151ec29c68c7, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"36ce8b88-3567-4564-bf17-8d134a9ea752"},{"cell_type":"code","source":["\n","from pyspark.sql import SparkSession\n","\n","from synapse.ml.core.platform import *\n","\n","# Bootstrap Spark Session\n","\n","spark = SparkSession.builder.getOrCreate()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"56a4ab0d-16a5-4a43-b9dd-151ec29c68c7","normalized_state":"finished","queued_time":"2024-08-16T15:50:48.9846757Z","session_start_time":null,"execution_start_time":"2024-08-16T15:50:50.4524214Z","execution_finish_time":"2024-08-16T15:50:50.7127063Z","parent_msg_id":"b666e603-dd86-4a21-86ae-1cadcb7559d0"},"text/plain":"StatementMeta(, 56a4ab0d-16a5-4a43-b9dd-151ec29c68c7, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fa2b21b1-fdb1-435e-8481-8d78bb163d77"},{"cell_type":"code","source":["from synapse.ml.cognitive import *\n","\n","# A general Azure AI services key for Text Analytics, Vision and Document Intelligence \n","service_key = \"<YOUR-KEY-VALUE>\" # Replace <YOUR-KEY-VALUE> with your Azure AI service key, check prerequisites for more details\n","\n","service_loc = \"eastus\"\n","\n","# or you could use separate keys that belong to each service\n","\n","# A Bing Search v7 subscription key\n","\n","bing_search_key =  \"<YOUR-KEY-VALUE>\" # Replace <YOUR-KEY-VALUE> with your Bing v7 subscription key, check prerequisites for more details\n","\n","# An Anomaly Detector subscription key\n","\n","anomaly_key = <\"YOUR-KEY-VALUE\"> # Replace <YOUR-KEY-VALUE> with your anomaly service key, check prerequisites for more details\n","\n","anomaly_loc = \"westus2\"\n","\n","# A Translator subscription key\n","\n","translator_key = \"<YOUR-KEY-VALUE>\" # Replace <YOUR-KEY-VALUE> with your translator service key, check prerequisites for more details\n","\n","translator_loc = \"eastus\"\n","\n","# An Azure search key\n","\n","search_key = \"<YOUR-KEY-VALUE>\" # Replace <YOUR-KEY-VALUE> with your search key, check prerequisites for more details"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"68f1c91a-1627-4801-9bd4-7d343996e511"},{"cell_type":"markdown","source":["## Task 2: Perform sentiment analysis on text"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a5e9a385-a9ad-4f18-b54a-43199a551554"},{"cell_type":"markdown","source":[],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1ce9067c-973f-4fca-94d9-4b3f1a666575"},{"cell_type":"code","source":["# Create a dataframe that's tied to it's column names\n","\n","df = spark.createDataFrame(\n","\n","    [\n","\n","        (\"I am so happy today, its sunny!\", \"en-US\"),\n","\n","        (\"I am frustrated by this rush hour traffic\", \"en-US\"),\n","\n","        (\"The cognitive services on spark aint bad\", \"en-US\"),\n","\n","    ],\n","\n","    [\"text\", \"language\"],\n","\n",")\n","\n","# Run the Text Analytics service with options\n","\n","sentiment = (\n","\n","    TextSentiment()\n","\n","    .setTextCol(\"text\")\n","\n","    .setLocation(service_loc)\n","\n","    .setSubscriptionKey(service_key)\n","\n","    .setOutputCol(\"sentiment\")\n","\n","    .setErrorCol(\"error\")\n","\n","    .setLanguageCol(\"language\")\n","\n",")\n","\n","# Show the results of your text query in a table format\n","\n","display(\n","\n","    sentiment.transform(df).select(\n","\n","        \"text\", col(\"sentiment.document.sentiment\").alias(\"sentiment\")\n","\n","    )\n","\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"3e53f39a-78b8-41f9-ad91-d9be7339a864"},{"cell_type":"markdown","source":["## Task 3: Perform text analytics for health data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1167af50-1285-44f6-aa82-4e213f13da45"},{"cell_type":"code","source":["df = spark.createDataFrame(\n","\n","    [\n","\n","        (\"20mg of ibuprofen twice a day\",),\n","\n","        (\"1tsp of Tylenol every 4 hours\",),\n","\n","        (\"6-drops of Vitamin B-12 every evening\",),\n","\n","    ],\n","\n","    [\"text\"],\n","\n",")\n","\n","healthcare = (\n","\n","    AnalyzeHealthText()\n","\n","    .setSubscriptionKey(service_key)\n","\n","    .setLocation(service_loc)\n","\n","    .setLanguage(\"en\")\n","\n","    .setOutputCol(\"response\")\n","\n",")\n","\n","display(healthcare.transform(df))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b364d741-9435-45f6-9e08-3ae8dc62397e"},{"cell_type":"markdown","source":["## Task 4: Translate text into a different language"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"90a6d40d-f67e-4bff-b147-61951e505db9"},{"cell_type":"code","source":["from pyspark.sql.functions import col, flatten, explode\n","\n","# Create a dataframe including sentences you want to translate\n","\n","df = spark.createDataFrame(\n","\n","    [([\"Hello, what is your name?\", \"Bye\"],)],\n","\n","    [\n","\n","        \"text\",\n","\n","    ],\n","\n",")\n","# Use the explode function to break down the array into different rows\n","df_exploded = df.withColumn(\"text\", explode(col(\"text\")))\n","\n","# Run the Translator service with options\n","\n","translate = (\n","\n","    Translate()\n","\n","    .setSubscriptionKey(service_key)\n","\n","    .setLocation(service_loc)\n","\n","    .setTextCol(\"text\")\n","\n","    .setToLanguage([\"de\", \"fr\", \"it\"]) # you can add multiple languages here\n","\n","    .setOutputCol(\"translation\")\n","\n",")\n","\n","# Show the results of the translation.\n","display(\n","    translate.transform(df_exploded)\n","    .withColumn(\"Translations\", flatten(col(\"translation.translations\")))\n","    .withColumn(\"GermanTranslation\", col(\"Translations\").getItem(0).getItem(\"text\"))\n","    .withColumn(\"FrenchTranslation\", col(\"Translations\").getItem(1).getItem(\"text\"))\n","    .withColumn(\"ItalianTranslation\", col(\"Translations\").getItem(2).getItem(\"text\"))\n","    .select(\"text\", \"Translations\",\"GermanTranslation\",\"FrenchTranslation\", \"ItalianTranslation\")\n",")\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"6822b471-4553-4e0c-9188-bc93a17b229e"},{"cell_type":"markdown","source":["## Task 5: Extract information from a document into structured data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1ea5ee37-81d7-45d9-bec5-68065ac51176"},{"cell_type":"markdown","source":["Azure AI Document Intelligence is a part of Azure AI services that lets you build automated data processing software using machine learning technology. With Azure AI Document Intelligence, you can identify and extract text, key/value pairs, selection marks, tables, and structure from your documents. The service outputs structured data that includes the relationships in the original file, bounding boxes, confidence and more."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3cbd9364-6fe7-4acd-b7f2-f7ba6239c167"},{"cell_type":"code","source":["from pyspark.sql.functions import col, explode\n","\n","# Create a dataframe containing the source files\n","\n","imageDf = spark.createDataFrame(\n","\n","    [\n","\n","        (\n","\n","            \"https://mmlspark.blob.core.windows.net/datasets/FormRecognizer/business_card.jpg\",\n","\n","        )\n","\n","    ],\n","\n","    [\n","\n","        \"source\",\n","\n","    ],\n","\n",")\n","\n","# Run the Form Recognizer service\n","\n","analyzeBusinessCards = (\n","\n","    AnalyzeBusinessCards()\n","\n","    .setSubscriptionKey(service_key)\n","\n","    .setLocation(service_loc)\n","\n","    .setImageUrlCol(\"source\")\n","\n","    .setOutputCol(\"businessCards\")\n","\n",")\n","\n","# Show the results of recognition.\n","\n","display(\n","\n","    analyzeBusinessCards.transform(imageDf)\n","\n","    .withColumn(\n","\n","        \"documents\", explode(col(\"businessCards.analyzeResult.documentResults.fields\"))\n","\n","    )\n","    .select(\"documents\")\n","\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f16f0071-8da2-4689-ac0d-b0205153085b"},{"cell_type":"markdown","source":["## Task 6: Analyze and tag images"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"119faac5-d8c0-4935-9d7d-c794defc75a7"},{"cell_type":"code","source":["# Create a dataframe with the image URLs\n","\n","base_url = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/\"\n","\n","df = spark.createDataFrame(\n","\n","    [\n","\n","        (base_url + \"objects.jpg\",),\n","\n","        (base_url + \"dog.jpg\",),\n","\n","        (base_url + \"house.jpg\",),\n","\n","    ],\n","\n","    [\n","\n","        \"image\",\n","\n","    ],\n","\n",")\n","\n","# Run the Computer Vision service. Analyze Image extracts information from/about the images.\n","\n","analysis = (\n","\n","    AnalyzeImage()\n","\n","    .setLocation(service_loc)\n","\n","    .setSubscriptionKey(service_key)\n","\n","    .setVisualFeatures(\n","\n","        [\"Categories\", \"Color\", \"Description\", \"Faces\", \"Objects\", \"Tags\"]\n","\n","    )\n","\n","    .setOutputCol(\"analysis_results\")\n","\n","    .setImageUrlCol(\"image\")\n","\n","    .setErrorCol(\"error\")\n","\n",")\n","\n","# Show the results of what you wanted to pull out of the images.\n","\n","display(analysis.transform(df).select(\"image\", \"analysis_results.description.tags\"))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"cd559d2e-3f6a-46cb-ad9c-29fb496d8d88"},{"cell_type":"markdown","source":["## Task 7: Search for images that are related to a natural language query\n","\n","issue here -> Skip this task"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3690f101-4b6e-47fd-990d-4558783b0a87"},{"cell_type":"code","source":["# Number of images Bing will return per query\n","\n","imgsPerBatch = 10\n","\n","# A list of offsets, used to page into the search results\n","\n","offsets = [(i * imgsPerBatch,) for i in range(100)]\n","\n","# Since web content is our data, we create a dataframe with options on that data: offsets\n","\n","bingParameters = spark.createDataFrame(offsets, [\"offset\"])\n","\n","# Run the Bing Image Search service with our text query\n","\n","bingSearch = (\n","\n","    BingImageSearch()\n","\n","    .setSubscriptionKey(bing_search_key)\n","\n","    .setOffsetCol(\"offset\")\n","\n","    .setQuery(\"Martin Luther King Jr. quotes\")\n","\n","    .setCount(imgsPerBatch)\n","\n","    .setOutputCol(\"images\")\n","\n",")\n","\n","# Transformer that extracts and flattens the richly structured output of Bing Image Search into a simple URL column\n","\n","getUrls = BingImageSearch.getUrlTransformer(\"images\", \"url\")\n","\n","# This displays the full results returned, uncomment to use\n","\n","display(bingSearch.transform(bingParameters))\n","\n","# Since we have two services, they are put into a pipeline\n","\n","pipeline = PipelineModel(stages=[bingSearch, getUrls])\n","\n","# Show the results of your search: image URLs\n","\n","display(pipeline.transform(bingParameters))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"cb29974c-cfb8-43ee-860c-c31fb6157f33"},{"cell_type":"markdown","source":["## Task 8: Transform speech to text"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"910d0d10-6024-45f1-bc48-afe3190ad2f9"},{"cell_type":"code","source":["# Create a dataframe with our audio URLs, tied to the column called \"url\"\n","\n","df = spark.createDataFrame(\n","\n","    [(\"https://mmlspark.blob.core.windows.net/datasets/Speech/audio2.wav\",)], [\"url\"]\n","\n",")\n","\n","# Run the Speech-to-text service to translate the audio into text\n","\n","speech_to_text = (\n","\n","    SpeechToTextSDK()\n","\n","    .setSubscriptionKey(service_key)\n","\n","    .setLocation(service_loc)\n","\n","    .setOutputCol(\"text\")\n","\n","    .setAudioDataCol(\"url\")\n","\n","    .setLanguage(\"en-US\")\n","\n","    .setProfanity(\"Masked\")\n","\n",")\n","\n","# Show the results of the translation\n","\n","display(speech_to_text.transform(df).select(\"url\", \"text.DisplayText\"))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"3bc80c62-05fe-49f2-830b-7615cfa1d891"},{"cell_type":"markdown","source":["## Task 9: Transform text to speech"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"82288c4a-ae7c-46e3-bffb-ed3fc0871c54"},{"cell_type":"code","source":["from synapse.ml.cognitive import TextToSpeech\n","\n","fs = \"\"\n","\n","if running_on_databricks():\n","\n","    fs = \"dbfs:\"\n","\n","elif running_on_synapse_internal():\n","\n","    fs = \"Files\"\n","\n","# Create a dataframe with text and an output file location\n","\n","df = spark.createDataFrame(\n","\n","    [\n","\n","        (\n","\n","            \"Reading out loud is fun! Check out aka.ms/spark for more information\",\n","\n","            fs + \"/output.mp3\",\n","\n","        )\n","\n","    ],\n","\n","    [\"text\", \"output_file\"],\n","\n",")\n","\n","tts = (\n","\n","    TextToSpeech()\n","\n","    .setSubscriptionKey(service_key)\n","\n","    .setTextCol(\"text\")\n","\n","    .setLocation(service_loc)\n","\n","    .setVoiceName(\"en-US-JennyNeural\")\n","\n","    .setOutputFileCol(\"output_file\")\n","\n",")\n","\n","# Check to make sure there were no errors during audio creation\n","\n","display(tts.transform(df))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"c079b6b9-8705-438c-bb7e-13edf2722440"},{"cell_type":"markdown","source":["## Task 10: Detect anomalies in time series data\n","\n","Anomaly Detector is great for detecting irregularities in your time series data. The following code sample uses the Anomaly Detector service to find anomalies in entire time series data."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c1a28b14-7ff9-49fb-b1eb-fb99c603dc13"},{"cell_type":"code","source":["# Create a dataframe with the point data that Anomaly Detector requires\n","\n","df = spark.createDataFrame(\n","\n","    [\n","\n","        (\"1972-01-01T00:00:00Z\", 826.0),\n","\n","        (\"1972-02-01T00:00:00Z\", 799.0),\n","\n","        (\"1972-03-01T00:00:00Z\", 890.0),\n","\n","        (\"1972-04-01T00:00:00Z\", 900.0),\n","\n","        (\"1972-05-01T00:00:00Z\", 766.0),\n","\n","        (\"1972-06-01T00:00:00Z\", 805.0),\n","\n","        (\"1972-07-01T00:00:00Z\", 821.0),\n","\n","        (\"1972-08-01T00:00:00Z\", 200000000.0),\n","\n","        (\"1972-09-01T00:00:00Z\", 883.0),\n","\n","        (\"1972-10-01T00:00:00Z\", 898.0),\n","\n","        (\"1972-11-01T00:00:00Z\", 957.0),\n","\n","        (\"1972-12-01T00:00:00Z\", 924.0),\n","\n","        (\"1973-01-01T00:00:00Z\", 881.0),\n","\n","        (\"1973-02-01T00:00:00Z\", 837.0),\n","\n","        (\"1973-03-01T00:00:00Z\", 9.0),\n","\n","    ],\n","\n","    [\"timestamp\", \"value\"],\n","\n",").withColumn(\"group\", lit(\"series1\"))\n","\n","# Run the Anomaly Detector service to look for irregular data\n","\n","anamoly_detector = (\n","\n","    SimpleDetectAnomalies()\n","\n","    .setSubscriptionKey(service_key)\n","\n","    .setLocation(service_loc)\n","\n","    .setTimestampCol(\"timestamp\")\n","\n","    .setValueCol(\"value\")\n","\n","    .setOutputCol(\"anomalies\")\n","\n","    .setGroupbyCol(\"group\")\n","\n","    .setGranularity(\"monthly\")\n","\n",")\n","\n","# Show the full results of the analysis with the anomalies marked as \"True\"\n","\n","display(\n","\n","    anamoly_detector.transform(df).select(\"timestamp\", \"value\", \"anomalies.isAnomaly\")\n","\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8ea93c5f-8f98-46c7-bc9d-f8018e1959c2"},{"cell_type":"markdown","source":["## Task 11: Get information from arbitrary web APIs\n","With HTTP on Spark, you can use any web service in your big data pipeline. The following code sample uses the World Bank API to get information about various countries around the world."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"aa0af45e-259c-47b8-bc68-eced7497dc22"},{"cell_type":"code","source":["# Use any requests from the python requests library\n","\n","def world_bank_request(country):\n","\n","    return Request(\n","\n","        \"GET\", \"http://api.worldbank.org/v2/country/{}?format=json\".format(country)\n","\n","    )\n","\n","# Create a dataframe with specifies which countries we want data on\n","\n","df = spark.createDataFrame([(\"br\",), (\"usa\",)], [\"country\"]).withColumn(\n","\n","    \"request\", http_udf(world_bank_request)(col(\"country\"))\n","\n",")\n","\n","# Much faster for big data because of the concurrency :)\n","\n","client = (\n","\n","    HTTPTransformer().setConcurrency(3).setInputCol(\"request\").setOutputCol(\"response\")\n","\n",")\n","\n","# Get the body of the response\n","\n","def get_response_body(resp):\n","\n","    return resp.entity.content.decode()\n","\n","# Show the details of the country data returned\n","\n","display(\n","\n","    client.transform(df).select(\n","\n","        \"country\", udf(get_response_body)(col(\"response\")).alias(\"response\")\n","\n","    )\n","\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"394aedf8-4d05-43e9-9af5-745c8dd9cbb0"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"d33b0f24-c944-4e79-a86b-500369025ce1","known_lakehouses":[{"id":"d33b0f24-c944-4e79-a86b-500369025ce1"}],"default_lakehouse_name":"AI_Services_Lab2_lh","default_lakehouse_workspace_id":"6e8ad812-370a-4955-9cd4-a4bf46973e00"}}},"nbformat":4,"nbformat_minor":5}